{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching YouTube Comments for COVID-19 Mask-Wearing Analysis (US)\n",
    "\n",
    "This notebook aims to fetch YouTube comments from videos posted by popular US news publishers (ABC News, CNN, Fox News) that discuss COVID-19. The comments will be analyzed for sentiment and emotional intensity related to mask-wearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define YouTube Comments Fetcher Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YouTubeCommentsFetcher:\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls, api_key: str):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(YouTubeCommentsFetcher, cls).__new__(cls)\n",
    "            cls._instance.api_key = api_key\n",
    "            cls._instance.youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "        return cls._instance\n",
    "\n",
    "    def _extract_video_id(self, url):\n",
    "        match = re.search(r'(?:v=)([a-zA-Z0-9_-]+)', url)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    def get_video_details(self, video_id):\n",
    "        video_request = self.youtube.videos().list(part=\"snippet\", id=video_id)\n",
    "        video_response = video_request.execute()\n",
    "        if video_response['items']:\n",
    "            snippet = video_response['items'][0]['snippet']\n",
    "            video_title = snippet['title']\n",
    "            news_publisher = snippet['channelTitle']\n",
    "            event_date = snippet['publishedAt'][:10]\n",
    "            return video_title, news_publisher, event_date\n",
    "        return None, None, None\n",
    "\n",
    "    def get_comments_for_video(self, video_url, country, include_replies=False):\n",
    "        video_id = self._extract_video_id(video_url)\n",
    "        if not video_id:\n",
    "            raise ValueError(\"Invalid video URL\")\n",
    "\n",
    "        video_title, news_publisher, event_date = self.get_video_details(video_id)\n",
    "        if not video_title:\n",
    "            raise ValueError(\"Unable to retrieve video details\")\n",
    "\n",
    "        if not (news_publisher in ['CNN', 'ABC News', 'Fox News']):\n",
    "            return []\n",
    "\n",
    "        all_comments = []\n",
    "        next_page_token = None\n",
    "\n",
    "        def contains_mask_keywords(comment):\n",
    "            comment_lower = str(comment).lower()\n",
    "            return \"wear\" in comment_lower and \"mask\" in comment_lower\n",
    "\n",
    "        while True:\n",
    "            comment_request = self.youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\",\n",
    "                maxResults=100\n",
    "            )\n",
    "            comment_response = comment_request.execute()\n",
    "\n",
    "            for item in comment_response['items']:\n",
    "                top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_text = top_comment['textDisplay']\n",
    "                comment_timestamp = top_comment['publishedAt'][:10]\n",
    "\n",
    "                if not contains_mask_keywords(comment_text):\n",
    "                    continue\n",
    "                if not ('2020-01-01' <= comment_timestamp <= '2022-12-31'):\n",
    "                    continue\n",
    "\n",
    "                all_comments.append({\n",
    "                    'country': country,\n",
    "                    'news_publisher': news_publisher,\n",
    "                    'event_date': event_date,\n",
    "                    'news_url': video_url,\n",
    "                    'news_title': video_title,\n",
    "                    'comment_text': comment_text,\n",
    "                    'comment_timestamp': comment_timestamp\n",
    "                })\n",
    "\n",
    "                if include_replies and item['snippet']['totalReplyCount'] > 0:\n",
    "                    all_comments.extend(self.get_replies(item['snippet']['topLevelComment']['id'], video_id, video_title, country, news_publisher, event_date, video_url))\n",
    "\n",
    "            next_page_token = comment_response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        return all_comments\n",
    "\n",
    "    def search_videos_by_channel_and_keyword(self, channel_id, keyword, max_videos=30):\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        video_count = 0\n",
    "\n",
    "        while video_count < max_videos:\n",
    "            search_request = self.youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                channelId=channel_id,\n",
    "                q=keyword,  \n",
    "                maxResults=50,  \n",
    "                pageToken=next_page_token,\n",
    "                type=\"video\"\n",
    "            )\n",
    "            search_response = search_request.execute()\n",
    "            search_videos = search_response['items']\n",
    "\n",
    "            for video in search_videos:\n",
    "                video_title = video['snippet']['title']\n",
    "                video_id = video['id']['videoId']\n",
    "                video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                videos.append({'video_id': video_id, 'video_title': video_title, 'video_url': video_url})\n",
    "                video_count += 1\n",
    "                if video_count >= max_videos:\n",
    "                    break\n",
    "\n",
    "            next_page_token = search_response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'YOUR_API_KEY'\n",
    "fetcher = YouTubeCommentsFetcher(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Channel IDs and Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_channel_id = \"YOUR_NEWS_CHANNEL_ID\"\n",
    "keyword = \"covid mask\"\n",
    "country = \"US\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Videos by Channel and Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_videos = fetcher.search_videos_by_channel_and_keyword(news_channel_id, keyword)\n",
    "searched_videos_urls = set(video['video_url'] for video in news_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'mask_comments_us.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    news_df = pd.read_csv(csv_file)\n",
    "    print('Original dataframe shape:', news_df.shape)\n",
    "    news_urls = news_df['news_url'].tolist()\n",
    "else:\n",
    "    news_df = pd.DataFrame(columns=['country', 'news_publisher', 'event_date', 'news_url', 'news_title', 'comment_text', 'comment_timestamp'])\n",
    "    news_df.to_csv(csv_file, index=False)\n",
    "    news_urls = []\n",
    "\n",
    "no_comments_file = 'junk_us_news_url.csv'\n",
    "if os.path.exists(no_comments_file):\n",
    "    no_comments_url_df = pd.read_csv(no_comments_file)\n",
    "else:\n",
    "    no_comments_url_df = pd.DataFrame(columns=['news_url'])\n",
    "\n",
    "number_of_fetched_comments = 0\n",
    "fetched_comments = []\n",
    "\n",
    "for video_url in searched_videos_urls:\n",
    "    if video_url in news_urls or video_url in no_comments_url_df['news_url'].tolist():\n",
    "        print(f'Comments for {video_url} already fetched. Skipping...')\n",
    "    else:\n",
    "        print(f'Fetching comments for {video_url}...')\n",
    "        try:\n",
    "            comments = fetcher.get_comments_for_video(video_url, country)\n",
    "        except Exception as e:\n",
    "            print(f'Comments for {video_url} are not accessible. Skipping...')\n",
    "            no_comments_url_df = pd.concat([no_comments_url_df, pd.DataFrame({'news_url': [video_url]})], ignore_index=True)\n",
    "            continue\n",
    "\n",
    "        if not comments:\n",
    "            print('No comments found.')\n",
    "            no_comments_url_df = pd.concat([no_comments_url_df, pd.DataFrame({'news_url': [video_url]})], ignore_index=True)\n",
    "        else:\n",
    "            new_comments_df = pd.DataFrame(comments)\n",
    "            new_comments_df['comment_text'] = new_comments_df['comment_text'].str.slice(0, 255)\n",
    "            new_comments_df['comment_timestamp'] = pd.to_datetime(new_comments_df['comment_timestamp'])\n",
    "\n",
    "            print(f'Number of comments found in {video_url}: {new_comments_df.shape[0]}')\n",
    "\n",
    "            number_of_fetched_comments += new_comments_df.shape[0]\n",
    "            fetched_comments.append(new_comments_df)\n",
    "\n",
    "no_comments_url_df.to_csv(no_comments_file, index=False)\n",
    "\n",
    "if fetched_comments:\n",
    "    print(\"Preview of new comments DataFrame:\")\n",
    "    all_new_comments_df = pd.concat(fetched_comments)\n",
    "    print(all_new_comments_df.shape)\n",
    "    print(all_new_comments_df.head())\n",
    "\n",
    "    add_comments = input(\"Do you want to add these comments to the existing DataFrame? (yes/no): \")\n",
    "    if add_comments.lower() == 'yes':\n",
    "        news_df = pd.concat([news_df, all_new_comments_df], ignore_index=True)\n",
    "        news_df.to_csv(csv_file, index=False)\n",
    "        print(f'Total number of new comments added: {number_of_fetched_comments}')\n",
    "    else:\n",
    "        print(\"No new comments added.\")\n",
    "else:\n",
    "    print(\"No new comments found.\")\n",
    "\n",
    "news_df = pd.read_csv(csv_file)\n",
    "news_df = news_df.drop_duplicates()\n",
    "print(\"Final dataframe shape:\", news_df.shape)\n",
    "news_df.to_csv(csv_file, index=False)\n",
    "print(\"Duplicates removed, final DataFrame saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
