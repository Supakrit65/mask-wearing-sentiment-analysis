{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thai YouTube Comments Fetching Notebook\n",
    "\n",
    "This notebook demonstrates how to fetch Thai YouTube comments from videos related to COVID-19 mask-wearing using various methods:\n",
    "1. Fetching videos by keyword\n",
    "2. Fetching videos by playlist URL\n",
    "3. Fetching videos by YouTube channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Youtube Comments Fetcher Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YouTubeCommentsFetcher:\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls, api_key: str):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(YouTubeCommentsFetcher, cls).__new__(cls)\n",
    "            cls._instance.api_key = api_key\n",
    "            cls._instance.youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "        return cls._instance\n",
    "\n",
    "    def _extract_playlist_id(self, url):\n",
    "        match = re.search(r'(?:list=)([a-zA-Z0-9_-]+)', url)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    def _extract_video_id(self, url):\n",
    "        match = re.search(r'(?:v=)([a-zA-Z0-9_-]+)', url)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    def get_video_title(self, video_id):\n",
    "        video_request = self.youtube.videos().list(\n",
    "            part=\"snippet\",\n",
    "            id=video_id\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "        return video_response['items'][0]['snippet']['title'] if video_response['items'] else None\n",
    "\n",
    "    def get_all_video_ids_from_playlists(self, playlist_urls):\n",
    "        playlist_ids = [self._extract_playlist_id(url) for url in playlist_urls]\n",
    "        playlist_ids = [pid for pid in playlist_ids if pid]\n",
    "        all_videos = []\n",
    "\n",
    "        for playlist_id in playlist_ids:\n",
    "            next_page_token = None\n",
    "            while True:\n",
    "                playlist_request = self.youtube.playlistItems().list(\n",
    "                    part='contentDetails',\n",
    "                    playlistId=playlist_id,\n",
    "                    maxResults=50,\n",
    "                    pageToken=next_page_token\n",
    "                )\n",
    "                playlist_response = playlist_request.execute()\n",
    "\n",
    "                all_videos += [item['contentDetails']['videoId'] for item in playlist_response['items']]\n",
    "                next_page_token = playlist_response.get('nextPageToken')\n",
    "\n",
    "                if not next_page_token:\n",
    "                    break\n",
    "\n",
    "        return all_videos\n",
    "\n",
    "    def get_video_details(self, video_id):\n",
    "        video_request = self.youtube.videos().list(\n",
    "            part=\"snippet\",\n",
    "            id=video_id\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "        if video_response['items']:\n",
    "            snippet = video_response['items'][0]['snippet']\n",
    "            video_title = snippet['title']\n",
    "            news_publisher = snippet['channelTitle']\n",
    "            event_year = snippet['publishedAt'][:10]\n",
    "            return video_title, news_publisher, event_year\n",
    "        return None, None, None\n",
    "\n",
    "    def get_comments_for_video(self, video_url, country, include_replies=False):\n",
    "        video_id = self._extract_video_id(video_url)\n",
    "        if not video_id:\n",
    "            raise ValueError(\"Invalid video URL\")\n",
    "\n",
    "        video_title, news_publisher, event_year = self.get_video_details(video_id)\n",
    "        if not video_title:\n",
    "            raise ValueError(\"Unable to retrieve video details\")\n",
    "\n",
    "        all_comments = []\n",
    "        next_page_token = None\n",
    "\n",
    "        mask_keywords = [\"mask\", \"wear mask\"]\n",
    "\n",
    "        if country == \"TH\":\n",
    "            mask_keywords = [\n",
    "                \"หน้ากาก\", \"มาส์ก\", \"ผ้าปิดหน้า\", \"แมส\",\n",
    "                \"มาส์ค\", \"การสวมหน้ากาก\", \"หน้ากากอนามัย\", \"แมสก์\"\n",
    "            ]\n",
    "\n",
    "            def contains_mask_keywords(comment):\n",
    "                tokens = word_tokenize(comment, engine='newmm', keep_whitespace=False)\n",
    "                return any(keyword in tokens for keyword in mask_keywords)\n",
    "        else:\n",
    "            def contains_mask_keywords(comment):\n",
    "                comment_lower = str(comment).lower()\n",
    "                return any(keyword in comment_lower for keyword in mask_keywords)\n",
    "\n",
    "        while True:\n",
    "            comment_request = self.youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\",\n",
    "                maxResults=100\n",
    "            )\n",
    "            comment_response = comment_request.execute()\n",
    "\n",
    "            for item in comment_response['items']:\n",
    "                top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_text = top_comment['textDisplay']\n",
    "\n",
    "                if not contains_mask_keywords(comment_text):\n",
    "                    continue\n",
    "\n",
    "                all_comments.append({\n",
    "                    'country': country,\n",
    "                    'news_publisher': news_publisher,\n",
    "                    'event_year': event_year,\n",
    "                    'news_url': video_url,\n",
    "                    'news_title': video_title,\n",
    "                    'comment_text': comment_text,\n",
    "                    'comment_timestamp': top_comment['publishedAt']\n",
    "                })\n",
    "\n",
    "                if include_replies and item['snippet']['totalReplyCount'] > 0:\n",
    "                    all_comments.extend(self.get_replies(item['snippet']['topLevelComment']['id'], video_id, video_title, country, news_publisher, event_year, video_url, mask_keywords))\n",
    "\n",
    "            next_page_token = comment_response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        return all_comments\n",
    "\n",
    "    def search_videos(self, keyword: str, country: str):\n",
    "        all_videos = []\n",
    "        next_page_token = None\n",
    "\n",
    "        while True:\n",
    "            search_request = self.youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                q=keyword,\n",
    "                regionCode=country,\n",
    "                type=\"video\",\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            search_response = search_request.execute()\n",
    "            for item in search_response['items']:\n",
    "                video_id = item['id']['videoId']\n",
    "                video_title = item['snippet']['title']\n",
    "                channel_title = item['snippet']['channelTitle']\n",
    "\n",
    "                keywords = [\"ข่าว\", \"top news\", \"spring news\", \"nationtv\", \"MCOT\", \"ช่อง\", \"thairath\", \"TNN\", \"Thai PBS\", \"เรื่องเล่า\", \"matichon tv\", \"สำนักข่าว\", \"voice\", \"amarin\", \"mono29\", \"ch7hd\", \"pptv\", \"Bright TV\", \"one31\"]\n",
    "                if any(keyword.lower() in channel_title.lower() for keyword in keywords):\n",
    "                    video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                    all_videos.append({'video_id': video_id, 'video_title': video_title, 'video_url': video_url})\n",
    "\n",
    "            next_page_token = search_response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        return all_videos\n",
    "\n",
    "    def get_channel_id_by_username(self, username):\n",
    "        request = self.youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=username,\n",
    "            type='channel'\n",
    "        )\n",
    "        response = request.execute()\n",
    "        if response['items']:\n",
    "            return response['items'][0]['snippet']['channelId']\n",
    "        return None\n",
    "\n",
    "    def get_all_videos_from_channel(self, channel_id):\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "\n",
    "        while True:\n",
    "            search_request = self.youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                channelId=channel_id,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token,\n",
    "                type=\"video\"\n",
    "            )\n",
    "            search_response = search_request.execute()\n",
    "            videos += search_response['items']\n",
    "            next_page_token = search_response.get('nextPageToken')\n",
    "\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        return videos\n",
    "\n",
    "    def search_videos_by_channel_and_keyword(self, channel_id, keyword):\n",
    "        videos = self.get_all_videos_from_channel(channel_id)\n",
    "        filtered_videos = []\n",
    "\n",
    "        for video in videos:\n",
    "            video_title = video['snippet']['title']\n",
    "            if keyword.lower() in video_title.lower():\n",
    "                video_id = video['id']['videoId']\n",
    "                video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                filtered_videos.append({'video_id': video_id, 'video_title': video_title, 'video_url': video_url})\n",
    "\n",
    "        return filtered_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'YOUR_API_KEY'\n",
    "fetcher = YouTubeCommentsFetcher(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Videos by Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"หน้ากากอนามัย\"\n",
    "country = \"TH\"\n",
    "\n",
    "videos_by_keyword = fetcher.search_videos(keyword, country)\n",
    "searched_videos_urls = set(video['video_url'] for video in videos_by_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Videos by Playlist URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_urls = [\n",
    "    'https://www.youtube.com/playlist?list=PLRS4T4F2sF1q9BL2_p0xNnDrkUlibC3Rd'\n",
    "]\n",
    "\n",
    "all_video_ids = fetcher.get_all_video_ids_from_playlists(playlist_urls)[:100]\n",
    "all_video_urls = [f\"https://www.youtube.com/watch?v={video_id}\" for video_id in all_video_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'mask_comments_th.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    news_df = pd.read_csv(csv_file)\n",
    "    news_urls = news_df['news_url'].tolist()\n",
    "else:\n",
    "    news_df = pd.DataFrame(columns=['country', 'news_publisher', 'event_date', 'news_url', 'news_title', 'comment_text', 'comment_timestamp'])\n",
    "    news_df.to_csv(csv_file, index=False)\n",
    "    news_urls = []\n",
    "\n",
    "number_of_new_comments = 0\n",
    "new_comments_dfs = []\n",
    "\n",
    "for video_url in all_video_urls:\n",
    "    if video_url in news_urls:\n",
    "        print(f'Comments for {video_url} already fetched.')\n",
    "    else:\n",
    "        print(f'Fetching comments for {video_url}...')\n",
    "        try:\n",
    "            comments = fetcher.get_comments_for_video(video_url, 'TH')\n",
    "        except Exception as e:\n",
    "            print(f'Comments for {video_url} are not accessible. Skipping...')\n",
    "            continue\n",
    "\n",
    "        if not comments:\n",
    "            print('No comments found.')\n",
    "        else:\n",
    "            new_comments_df = pd.DataFrame(comments)\n",
    "            new_comments_df['comment_text'] = new_comments_df['comment_text'].str.slice(0, 255)\n",
    "            new_comments_df['comment_timestamp'] = pd.to_datetime(new_comments_df['comment_timestamp'])\n",
    "\n",
    "            print(f'Number of comments found in {video_url}: {new_comments_df.shape[0]}')\n",
    "\n",
    "            number_of_new_comments += new_comments_df.shape[0]\n",
    "            new_comments_dfs.append(new_comments_df)\n",
    "\n",
    "if new_comments_dfs:\n",
    "    print(\"Preview of new comments DataFrame:\")\n",
    "    all_new_comments_df = pd.concat(new_comments_dfs)\n",
    "    print(all_new_comments_df.shape)\n",
    "    print(all_new_comments_df.head())\n",
    "\n",
    "    add_comments = input(\"Do you want to add these comments to the existing DataFrame? (yes/no): \")\n",
    "    if add_comments.lower() == 'yes':\n",
    "        news_df = pd.concat([news_df, all_new_comments_df], ignore_index=True)\n",
    "        news_df.to_csv(csv_file, index=False)\n",
    "        print(f'Total number of new comments added: {number_of_new_comments}')\n",
    "    else:\n",
    "        print(\"No new comments added.\")\n",
    "else:\n",
    "    print(\"No new comments found.\")\n",
    "\n",
    "thai_comments = pd.read_csv(csv_file)\n",
    "thai_comments = thai_comments.drop_duplicates()\n",
    "thai_comments.to_csv(csv_file, index=False)\n",
    "thai_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Videos by Youtube Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_username = '@ThaiPBS'\n",
    "channel_id = fetcher.get_channel_id_by_username(channel_username)\n",
    "print(f\"Channel ID for {channel_username}: {channel_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'โควิด หน้ากาก'\n",
    "search_results = fetcher.search_videos_by_channel_and_keyword(channel_id, keyword)\n",
    "thai_news_urls = set([result['video_url'] for result in search_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'mask_comments_th.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    news_df = pd.read_csv(csv_file)\n",
    "    news_urls = news_df['news_url'].tolist()\n",
    "else:\n",
    "    news_df = pd.DataFrame(columns=['country', 'news_publisher', 'event_date', 'news_url', 'news_title', 'comment_text', 'comment_timestamp'])\n",
    "    news_df.to_csv(csv_file, index=False)\n",
    "    news_urls = []\n",
    "\n",
    "number_of_new_comments = 0\n",
    "new_comments_dfs = []\n",
    "\n",
    "for video_url in thai_news_urls:\n",
    "    if video_url in news_urls:\n",
    "        print(f'Comments for {video_url} already fetched.')\n",
    "    else:\n",
    "        print(f'Fetching comments for {video_url}...')\n",
    "        try:\n",
    "            comments = fetcher.get_comments_for_video(video_url, 'TH')\n",
    "        except Exception as e:\n",
    "            print(f'Comments for {video_url} are not accessible. Skipping...')\n",
    "            continue\n",
    "\n",
    "        if not comments:\n",
    "            print('No comments found.')\n",
    "        else:\n",
    "            new_comments_df = pd.DataFrame(comments)\n",
    "            new_comments_df['comment_text'] = new_comments_df['comment_text'].str.slice(0, 255)\n",
    "            new_comments_df['comment_timestamp'] = pd.to_datetime(new_comments_df['comment_timestamp'])\n",
    "\n",
    "            print(f'Number of comments found in {video_url}: {new_comments_df.shape[0]}')\n",
    "\n",
    "            number_of_new_comments += new_comments_df.shape[0]\n",
    "            new_comments_dfs.append(new_comments_df)\n",
    "\n",
    "if new_comments_dfs:\n",
    "    print(\"Preview of new comments DataFrame:\")\n",
    "    all_new_comments_df = pd.concat(new_comments_dfs)\n",
    "    print(all_new_comments_df.shape)\n",
    "    print(all_new_comments_df.head())\n",
    "\n",
    "    add_comments = input(\"Do you want to add these comments to the existing DataFrame? (yes/no): \")\n",
    "    if add_comments.lower() == 'yes':\n",
    "        news_df = pd.concat([news_df, all_new_comments_df], ignore_index=True)\n",
    "        news_df.to_csv(csv_file, index=False)\n",
    "        print(f'Total number of new comments added: {number_of_new_comments}')\n",
    "    else:\n",
    "        print(\"No new comments added.\")\n",
    "else:\n",
    "    print(\"No new comments found.\")\n",
    "\n",
    "thai_comments = pd.read_csv(csv_file)\n",
    "thai_comments = thai_comments.drop_duplicates()\n",
    "thai_comments.to_csv(csv_file, index=False)\n",
    "thai_comments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
